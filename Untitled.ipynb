{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep  7 17:12:32 2020\n",
    "\n",
    "@author: ShuangZhao\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHyperparameters:\\n    HL: Number of layers in SDF Network, 2,3,4  | 2\\n    HU: Number of hidden units in SDF Networks 64\\n    SMV: Number of hidden states in SDF Networks 4, 8  |4\\n    CHL: Number of layers in Conditional Network, 0,1  |0\\n    CHU: Number of hidden units in Conditional Networks 4,8,16,32  |8\\n    CSMV: Number of hidden states in Conditional Networks 16,32  |32\\n    LR: Initial learning rate 0.001,0.0005,0.0002,0.0001  |0.01\\n    DR: Dropout 0.95\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#%%\n",
    "# generate simulation data\n",
    "T_train, T_valid, T_test = 250, 100, 250\n",
    "N = 500\n",
    "T = T_train + T_valid + T_test\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence,time_steps):\n",
    "    x = list()\n",
    "    for i in range(time_steps):\n",
    "        seq_x = sequence[:i+1]\n",
    "        x.append(seq_x)\n",
    "    for i in range(time_steps,len(sequence)):\n",
    "        seq_x = sequence[i-time_steps+1:i+1]\n",
    "        x.append(seq_x)   \n",
    "    return tf.ragged.constant(x)\n",
    "\n",
    "def get_sequence(sequence):\n",
    "    x = list()\n",
    "    for i in range(len(sequence)):\n",
    "        seq_x = sequence[:i+1]\n",
    "        x.append(seq_x)   \n",
    "    return tf.ragged.constant(x)\n",
    "\n",
    "# two firm characteristics\n",
    "def get_data_2(mu_F,sigma_F,mu_epsilon,sigma_epsilon):\n",
    "    firm_char = np.random.normal(0,1,size = (T,N,2))\n",
    "    C_1 = firm_char[:,:,0]\n",
    "    C_2 = firm_char[:,:,1]\n",
    "    beta = C_1*C_2\n",
    "    epsilon = np.random.normal(mu_epsilon,sigma_epsilon, size = (T,N))\n",
    "    F = np.random.normal(mu_F,sigma_F, size = (T,N))\n",
    "    R = beta*F+epsilon\n",
    "    return firm_char, F, R\n",
    "\n",
    "data_1 = get_data_2(np.sqrt(0.1), np.sqrt(0.1), 0, 1)\n",
    "\n",
    "# one macro and one firm charateristic\n",
    "def get_data_1_1(mu_F,sigma_F,mu_epsilon,sigma_epsilon,mu_h,sigma_h,mu_M):\n",
    "    # construct macro factor\n",
    "    epsilon_h = np.random.normal(mu_h,sigma_h,size = (T,1))\n",
    "    t= np.array(range(T))\n",
    "    h = np.sin(np.pi*t/24).reshape(-1,1) + epsilon_h\n",
    "    Z = (mu_M*t).reshape(-1,1) + h\n",
    "    b_h = lambda x: np.sign(x) if x!= 0 else -1\n",
    "    b = np.array([b_h(x) for x in h])\n",
    "    # firm characteristic\n",
    "    firm_char = np.random.normal(0,1,size = (T,N,1))\n",
    "    # construct data\n",
    "    C_1 = firm_char[::,0]\n",
    "    beta = C_1 * np.dot(b,np.ones([1,N]))\n",
    "    epsilon = np.random.normal(mu_epsilon,sigma_epsilon, size = (T,N))\n",
    "    F = np.random.normal(mu_F,sigma_F, size = (T,N))\n",
    "    R = beta*F+epsilon\n",
    "    z = np.vstack([0,np.diff(Z,axis=0)])\n",
    "    return firm_char, get_sequence(z), Z, h, F, R\n",
    "\n",
    "data_2 = get_data_1_1(np.sqrt(0.1), np.sqrt(0.1), 0, 1, 0, np.sqrt(0.25), 0.05)\n",
    "\n",
    "\n",
    "#%% Hyperparameters\n",
    "'''\n",
    "plt.rcParams['figure.figsize'] = (12, 4.0)\n",
    "plt.figure()\n",
    "plt.scatter(t,h)\n",
    "plt.show()\n",
    "'''\n",
    "'''\n",
    "Hyperparameters:\n",
    "    HL: Number of layers in SDF Network, 2,3,4  | 2\n",
    "    HU: Number of hidden units in SDF Networks 64\n",
    "    SMV: Number of hidden states in SDF Networks 4, 8  |4\n",
    "    CHL: Number of layers in Conditional Network, 0,1  |0\n",
    "    CHU: Number of hidden units in Conditional Networks 4,8,16,32  |8\n",
    "    CSMV: Number of hidden states in Conditional Networks 16,32  |32\n",
    "    LR: Initial learning rate 0.001,0.0005,0.0002,0.0001  |0.01\n",
    "    DR: Dropout 0.95\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Build network structure\n",
    "\n",
    "HL = 2\n",
    "HU = 64\n",
    "SMV = 4\n",
    "DR = 0.95\n",
    "n_macro =1\n",
    "n_firm = 1\n",
    "macro_inputs = keras.Input(250,1)\n",
    "\n",
    "\n",
    "\n",
    "def build_model(HL, HU, SMV, DR, n_macro, n_firm):\n",
    "    firm_inputs = keras.Input(shape=(N, n_firm,))\n",
    "    macro_inputs = keras.Input(shape = (None,n_macro,))\n",
    "    inputs = [firm_inputs, macro_inputs]\n",
    "\n",
    "    # layers\n",
    "    lstm = tf.keras.layers.LSTM(SMV,return_state = True,input_shape = (None, n_macro))\n",
    "    model = keras.Sequential()\n",
    "    for i in range(HL):\n",
    "        model.add(layers.Dense(HU,activation = 'relu'))\n",
    "        model.add(layers.Dropout(DR))\n",
    "    model.add(layers.Dense(1,activation = 'linear'))\n",
    "    \n",
    "    # get_macro_states\n",
    "  #  macro_states = tf.reshape(inputs[1])\n",
    "    # get the hidden state at the final time_step\n",
    "    macro_states = lstm(inputs[1])[1]\n",
    "    macro_states = tf.reshape(macro_states,[-1,1,SMV])\n",
    "    macro_states = tf.tile(macro_states,[1,N,1])\n",
    "\n",
    "    input_FNN = tf.concat([inputs[0], macro_states],axis=2)\n",
    "    outputs = model(input_FNN)\n",
    "    FNN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return FNN\n",
    "\n",
    "def build_conditional_model(HL, HU, CHU, SMV, DR, n_macro, n_firm):\n",
    "    firm_inputs = keras.Input(shape=(N, n_firm,))\n",
    "    macro_inputs = keras.Input(shape = (n_macro,))\n",
    "    inputs = [firm_inputs, macro_inputs]\n",
    "\n",
    "    # layers\n",
    "    lstm = tf.keras.layers.LSTM(SMV,input_shape = (None, n_macro))\n",
    "    lstm = tf.keras.layers.LSTM(SMV,return_sequences = True)\n",
    "    model = keras.Sequential()\n",
    "    for i in range(HL):\n",
    "        model.add(layers.Dense(HU,activation = 'relu'))\n",
    "        model.add(layers.Dropout(DR))\n",
    "    model.add(layers.Dense(CHU,activation = 'linear'))\n",
    "    \n",
    "    # get_macro_states\n",
    "    macro_states = tf.reshape(inputs[1],[1,-1,n_macro])\n",
    "    macro_states = lstm(macro_states)\n",
    "    macro_states = tf.reshape(macro_states,[-1,1,SMV])\n",
    "    macro_states = tf.tile(macro_states,[1,N,1])\n",
    "\n",
    "    input_FNN = tf.concat([inputs[0], macro_states],axis=2)\n",
    "    outputs = model(input_FNN)\n",
    "    FNN = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return FNN\n",
    "\n",
    "# FNN for beta \n",
    "\n",
    "#%% define loss function\n",
    "\n",
    "def get_SDF_loss(g):\n",
    "    CHU = g.shape[-1]\n",
    "    g = tf.reshape(g, [-1,N,1,CHU])\n",
    "    def loss(R,w):\n",
    "        M = 1 - tf.reduce_sum(tf.multiply(w,R),1)\n",
    "        M = tf.tile(tf.reshape(M,[-1,1,1,1]), [1,N,1,CHU])\n",
    "        R = tf.tile(tf.reshape(R,[-1,N,1,1]),[1,1,1,CHU])\n",
    "        diff = tf.multiply(M,R,g)\n",
    "        l = tf.reduce_sum(tf.reduce_sum(tf.square(diff),[0,2,3]))/N\n",
    "        return l\n",
    "    return loss\n",
    "\n",
    "def get_Condition_loss(w):\n",
    "    def loss(R,g):\n",
    "        M = 1 - tf.reduce_sum(tf.multiply(w,R),1)\n",
    "        diff = M*R*g\n",
    "        l = tf.reduce_sum(tf.square(tf.norm(diff,ord = 'euclidean', axis= [0,2])))/N\n",
    "        return l\n",
    "    return loss\n",
    "\n",
    "def unconditional_loss(R,w):\n",
    "    M = 1 - tf.reduce_sum(tf.multiply(w,R),1)\n",
    "    M = tf.tile(tf.reshape(M,[-1,1,1,1]), [1,N,1,1])\n",
    "    R = tf.tile(tf.reshape(R,[-1,N,1,1]),[1,1,1,1])\n",
    "    diff = tf.multiply(M,R)\n",
    "    l = tf.reduce_sum(tf.reduce_sum(tf.square(diff),[0,2,3]))/N\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training\n",
    "# step 1\n",
    "n_firm = 1\n",
    "n_macro = 1\n",
    "x_train = [(data_2[0][:T_train]).reshape(-1,N,n_firm),(data_2[1][:T_train])]\n",
    "y_train = data_2[4][:T_train].reshape(-1,N,1)\n",
    "\n",
    "SDF = build_model(HL = 2, HU = 64, SMV = 4, DR = 0.05, n_macro =1, n_firm = 1)\n",
    "SDF.summary()\n",
    "SDF.compile(optimizer = 'adam', loss = unconditional_loss)\n",
    "history1 = SDF.fit(x_train, y_train,epochs=500)\n",
    "plt.plot(history1.history['loss'])\n",
    "\n",
    "w = SDF.predict(x_train)\n",
    "'''\n",
    "# Build conditional network\n",
    "Condition = build_conditional_model(HL = 0, HU = 64, CHU = 8, SMV = 32, DR = 0.95, n_macro =1, n_firm = 1)\n",
    "Condition.summary()\n",
    "'''\n",
    "\n",
    "state = SDF.layers[1]\n",
    "macro_index = data_2[1]\n",
    "macro = state(macro_index)[1].numpy()\n",
    "\n",
    "t = np.array(range(600))\n",
    "plt.rcParams['figure.figsize'] = (12, 4.0)\n",
    "plt.figure()\n",
    "plt.scatter(t,macro[:,3])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 4)                 96        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([250, None, None]), tf.float64, 2, tf.int64)\n\nSecond structure: type=Tensor str=Tensor(\"lstm_input:0\", shape=(None, None, 1), dtype=float32)\n\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([250, None, None]), tf.float64, 2, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"lstm_input:0\", shape=(None, None, 1), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    329\u001b[0m     _pywrap_utils.AssertSameStructure(nest1, nest2, check_types,\n\u001b[1;32m--> 330\u001b[1;33m                                       expand_composites)\n\u001b[0m\u001b[0;32m    331\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([250, None, None]), tf.float64, 2, tf.int64)\n\nSecond structure: type=Tensor str=Tensor(\"lstm_input:0\", shape=(None, None, 1), dtype=float32)\n\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([250, None, None]), tf.float64, 2, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"lstm_input:0\", shape=(None, None, 1), dtype=float32)\" is not",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5e42208c0b08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mT_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mT_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[0mflat_expected_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_expected_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2447\u001b[1;33m       \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2449\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\SongyanHou\\Programs\\Anaconda3\\envs\\justin\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    335\u001b[0m                   \u001b[1;34m\"Entire first structure:\\n%s\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                   \u001b[1;34m\"Entire second structure:\\n%s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m                   % (str(e), str1, str2))\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([250, None, None]), tf.float64, 2, tf.int64)\n\nSecond structure: type=Tensor str=Tensor(\"lstm_input:0\", shape=(None, None, 1), dtype=float32)\n\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([250, None, None]), tf.float64, 2, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"lstm_input:0\", shape=(None, None, 1), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n."
     ]
    }
   ],
   "source": [
    "\n",
    "#%% test LSTM\n",
    "z = data_2[1]\n",
    "h = data_2[3]\n",
    "\n",
    "'''\n",
    "input_1 = keras.Input(shape = [None,1,],name = 'input')\n",
    "lstm = keras.layers.LSTM(4, activation='relu', input_shape=(None,1), return_state = True,name = 'lstm')\n",
    "out = layers.Dense(1,name = 'out')\n",
    "model = keras.Model(inputs =input_1, outputs = out(lstm(input_1)[1]), name = 'RNN')\n",
    "model.summary()\n",
    "# define model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(4, input_shape = (None,1),return_sequences = True))\n",
    "#model.add(layers.Dense(1,activation = 'linear'))\n",
    "model.summary()\n",
    "'''\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(4,input_shape=(None,1)))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "model.fit(z[:T_train], h[:T_train],epochs =300)\n",
    "\n",
    "t = np.array(range(T))\n",
    "pre_h = (model.layers[0])(z)\n",
    "pre = model.predict(z)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(t,h,label='True')\n",
    "plt.scatter(t[:T_train],pre[:T_train],label = 'train')\n",
    "plt.scatter(t[T_train:T_train+T_valid],pre[T_train:T_train+T_valid],label = 'valid')\n",
    "plt.scatter(t[(T_train+T_valid):],pre[(T_train+T_valid):],label = 'train')\n",
    "plt.legend()\n",
    "\n",
    "plt.scatter(t,pre_h.numpy()[:,0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
